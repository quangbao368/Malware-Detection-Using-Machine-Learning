import pandas as pd
import numpy as np
import logging
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.feature_selection import SelectFromModel
import matplotlib.pyplot as plt

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load your dataset
def load_data(filepath):
    logging.info("Loading data...")
    data = pd.read_csv(filepath)
    return data

# Feature selection
def select_features(X_train, y_train):
    logging.info("Selecting features...")
    clf = RandomForestClassifier(n_estimators=100)
    clf = clf.fit(X_train, y_train)
    model = SelectFromModel(clf, prefit=True)
    X_new = model.transform(X_train)
    feature_idx = model.get_support()
    feature_name = X_train.columns[feature_idx]
    logging.info(f"Selected features: {feature_name}")
    return X_new, feature_name

# Train model with Grid Search
def train_model_with_grid_search(X_train, y_train):
    logging.info("Training model with Grid Search...")
    param_grid = {
        'n_estimators': [100, 200],
        'max_features': ['auto', 'sqrt', 'log2'],
        'max_depth' : [4, 5, 6, 7, 8],
        'criterion' :['gini', 'entropy']
    }
    rf_model = RandomForestClassifier()
    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv= 5)
    grid_search.fit(X_train, y_train)
    logging.info(f"Best parameters: {grid_search.best_params_}")
    return grid_search.best_estimator_

# Evaluate model
def evaluate_model(model, X_test, y_test):
    logging.info("Evaluating model...")
    predictions = model.predict(X_test)
    probs = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, predictions)
    clf_report = classification_report(y_test, predictions)
    conf_matrix = confusion_matrix(y_test, predictions)
    roc_auc = roc_auc_score(y_test, probs)
    fpr, tpr, thresholds = roc_curve(y_test, probs)
    return accuracy, clf_report, conf_matrix, roc_auc, fpr, tpr

# Plot ROC Curve
def plot_roc_curve(fpr, tpr, roc_auc):
    logging.info("Plotting ROC curve...")
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()

# Main function to run the workflow
def main():
    data = load_data('full.csv')
    X, y = data.drop('label', axis=1), data['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    X_train_new, selected_features = select_features(X_train, y_train)
    X_test_new = X_test[selected_features]

    model = train_model_with_grid_search(X_train_new, y_train)
    accuracy, report, matrix, roc_auc, fpr, tpr = evaluate_model(model, X_test_new, y_test)

    logging.info(f"Accuracy: {accuracy}")
    logging.info(f"Classification Report:\n{report}")
    logging.info(f"Confusion Matrix:\n{matrix}")
    logging.info(f"ROC AUC: {roc_auc}")

    plot_roc_curve(fpr, tpr, roc_auc)

if __name__ == '__main__':
    main()
